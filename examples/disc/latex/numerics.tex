%\documentclass[12pt,a4paper]{amsart}
%!TEX TS-options = --shell-escape
\pdfobjcompresslevel=0% fix PDF inclusions
\documentclass[draft,
a4paper,11pt,DIV=11,% decrease borders by 2 (std 10 for 11pt)
abstract=on% return to old style centered abstract
]{scrartcl}
%
\usepackage{algorithm, booktabs}
\usepackage{mathtools}
\mathtoolsset{showonlyrefs}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{faktor}
\usepackage{amscd}
\usepackage{array}
%\usepackage{refcheck}\norefnames
\usepackage{amsthm}
\usepackage{subfigure}
\usepackage{mathdots}
\usepackage{mathrsfs}
\usepackage[noend]{algpseudocode}
\usepackage{graphicx}
\usepackage{color}
\usepackage{pgf}
\usepackage{scrpage2}
\usepackage{multirow}
\usepackage{listings}
\lstset{language=matlab,showstringspaces=false,basicstyle={\ttfamily}}
\usepackage{tikz}
\usepackage{graphicx}
\usepackage[left=2.5cm, right=2.5cm,top=2.5cm,bottom=2.5cm]{geometry}
\usepackage{scalerel}

\DeclareMathOperator{\Lcurve}{L-curv}
\newcommand{\TSP}{\text{TSP}}


\newtheorem{thm}{Theorem}[section]
\newtheorem{lemma}[thm]{Lemma}
\newtheorem{remark}[thm]{Remark}
\newtheorem{definition}[thm]{Definition}
\newtheorem{example}[thm]{Example}
\newtheorem{corollary}[thm]{Corollary}
\newtheorem{proposition}[thm]{Proposition}

%---------------------------------------------------------------------------
\begin{document}
\title{Curve Based Approximation of Measures on Manifolds by Discrepancy Minimization}

\author{
Martin Ehler\footnotemark[1],
\and
Manuel Gr\"af\footnotemark[2],
\and
Sebastian Neumayer\footnotemark[3]
\and
and Gabriele Steidl\footnotemark[3]
}
\maketitle

\date{\today}


\footnotetext[3]{Department of Mathematics,
	TU Kaiserslautern,
	Paul-Ehrlich-Str.~31, D-67663 Kaiserslautern, Germany,
	\{neumayer,steidl\}@mathematik.uni-kl.de
	}
\footnotetext[1]{University of Vienna, Department of Mathematics, Vienna, Austria,
	\{martin.ehler\}@univie.ac.at
	}
\footnotetext[2]{Austrian Academy of Sciences, Acoustics Research Institute, Vienna, Austria,
	\{mgraef\}@kfs.oeaw.ac.at
	}

\begin{abstract}
	\noindent\small
	

\end{abstract}

\section{Main problem}

We like to solve
\begin{equation}
  \label{eq:min_PL}
  \inf_{\nu} \mathscr D(\nu,\mu) \qquad \text{subject to} \qquad \nu \in \mathcal P_{L}^{\Lcurve}(\mathbb X).
\end{equation}
%respectively the relaxed optimization problem
%\begin{equation}
%  \label{eq:min_PL_relaxed}
%  \min_{\gamma \in A_{L}} \mathscr D^{2}(\gamma_{*}(\mathrm dt),\mu) + \alpha \int_{0}^{1} (\|\dot \gamma(t)\|-L)_{+}^{2} \mathrm dt.
%\end{equation}

\section{Numerical Approach}

For fixed $L>0$ and probability measure $\mu$ on $\mathbb X$ we aim to solve the minimization problem
\begin{equation}
  \label{eq:constant_speed_min}
  \min_{x_{1},\dots,x_{N}} \mathscr{D}^{2} \Big(\mu, \frac{1}{N} \sum_{k=1}^{N} \delta_{x_{k}}\Big) \qquad \mathrm{s.t.} \qquad    N \mathrm{dist}(x_{k},x_{k-1}) \le L, \quad k=1,\dots,N.
\end{equation}
Letting $N$ tend to infinity we eventually approach a closed curve $\gamma$, $\gamma(\tfrac kN) \approx x_{k}$ of speed at most $L$ such that the push-forward $\nu = \gamma_{*}(\mathrm dt)$, approaches $\mu$.  

We shall solve the optimization problem approximately by applying the nonlinear conjugate gradient method to the following unconstrained formulation
\begin{equation}
  \label{eq:constant_speed_min_relaxed}
  \min_{x_{1},\dots,x_{N}} \mathscr{D}^{2} \Big(\mu, \frac{1}{N} \sum_{k=1}^{N} \delta_{x_{k}}\Big) +   \frac{\alpha}{N} \sum_{k=1}^{N} \big(N \mathrm{dist}(x_{k-1},x_{k}) - L \big)_{+}^{2}
\end{equation}
where $\alpha > 0$ is a penalty parameter. Note, for $\alpha \to \infty$ there is a subsequence of minimizers of \eqref{eq:constant_speed_min_relaxed} which will converge to minimizers of \eqref{eq:constant_speed_min}.

\begin{remark}
  Obviously the objective function in \eqref{eq:constant_speed_min_relaxed} is not once and twice differentiable whenever $x_{k-1}=x_{k}$ and $\mathrm{dist}(x_{k-1},x_{k}) = \frac{L}{N}$, respectively. However, we observe numerically that local minimizers of \eqref{eq:constant_speed_min_relaxed} do not belong to this critical set of measure zero. That means in turn, if a local minimizer has a positive definite Hessian, then there is a local neighborhood where the CG method (with exact line search) permits a superlinearly convergence rate. We do indeed observe this behavior.
\end{remark}
For large $L$ we cannot hope to find a solution/global minimizer to problem \eqref{eq:constant_speed_min_relaxed}, since the number of local minimizers appears to increase exponentially {\color{blue}[ ref???]}.

\subsection{Numerical evaluation}
Recall that for given kernels $K:\mathbb X\times \mathbb X \to \mathbb R$ the squared discrepancy can be evaluated by
\[
  \mathscr D^{2}\Big(\mu, \frac 1N \sum_{k=1}^{N} \delta_{x_{k}}\Big) = E_{\mu}(x_{1},\dots,x_{N})
\]
where
\begin{equation}
  \label{eq:E_mu}
  E_{\mu}(x_{1},\dots,x_{N}) :=
    \frac{1}{N^{2}}\sum_{i,j=1}^{N} K(x_{i},x_{j}) - 2\sum_{i=1}^{N} \int_{\mathbb X} K(x_{i},x) \mathrm d\mu(x) + \int_{\mathbb X}\int_{\mathbb X} K(x,y) \mathrm d\mu(x) \mathrm d\mu(y).
  \end{equation}
Using the eigenfunction expansion
\[
 K(x,y) = \sum_{l=0}^{\infty} \lambda_{l} \varphi_{l}(x) \overline{\varphi_{l}(y)} 
\]
we have the alternative evaluation formula
\begin{equation}
  \label{eq:E_mu_Fourier}
  E_{\mu}(x_{1},\dots,x_{N}) = \sum_{l=0}^{\infty} \lambda_{l} \Big| \hat\mu_{l}
  - \frac 1N \sum_{i=1}^{N} \varphi_{l}(x_{i}) \Big|^{2}, \quad\quad \mu_{l} := \int_{\mathbb X} \varphi_{l}(x) \mathrm d\mu(x).
\end{equation}
Both formulas have it pros and cons:
\begin{itemize}
\item The formula \eqref{eq:E_mu} allows for exact evaluation only if the expression
\[
  K_{\mu}(x) := \int_{\mathbb X} K(x,y) \mathrm d\mu(y), \qquad  c_{\mu} := \int_{\mathbb X} K_{\mu}(x) \mathrm d\mu(x),
\]
can be written in closed form. In that case the complexity scales quadratically in the number $N$ of points.

\item The formula \eqref{eq:E_mu_Fourier} allows for exact evaluation only if the kernel has a finite expansion. In that case the complexity scales linearly in the number $N$ of points. 
\end{itemize}
Our approach is to approximate the kernel $K$ by a truncation
\[
  K_{t}(x,y) := \sum_{l=0}^{t} \lambda_{l} \varphi_{l}(x) \overline{\varphi_{l}(y)}
\]
with $t \sim N$ and evaluate formula \eqref{eq:E_mu_Fourier} by fast summation methods, so that the overall complexity is almost linearly in $t$ and $N$. 

\subsection{Choice of Parameters}

In order to find reasonable (local) solutions of \eqref{eq:min_PL} we shall carefully adjust the parameters in problem \eqref{eq:constant_speed_min_relaxed}, namely the penalty parameter $\alpha$, the number of points $N$ and the polynomial degree $t$.

\textbf{Number of Points.}
From the asymptotic of the path lengths of the traveling sales man problem (TSP) {\color{blue}(see aslo asymptotic for atomic/empirical measures)}
we know that we should choose
\[
  N \gtrsim \mathcal L(\gamma)^{\frac{d}{d-1}},
\]
where
\[
  \mathcal L(\gamma) = \sum_{k=1}^{N} \mathrm{dist}(x_{k-1},x_{k}) \le L
\]
is the length of the resulting curve $\gamma$ going through the points, i.e.,
\[
 \gamma(t_{k}) := x_{k}, \qquad t_{k} = \sum_{i=1}^{k}  \mathrm{dist}(x_{i-1},x_{i})/ \mathcal L(\gamma).
\]

%In Figure~\ref{fig:Nexperiment} we show some results for the two-dimensional torus. 

\textbf{Polynomial degree.} We believe it is reasonable to take $t \sim N^{\frac1d} \sim L^{\frac{1}{d-1}}$. Note, in that case the computational cost is almost linearly in $N$, for $\mathbb X = \mathbb T^{d}, \mathbb S^{2}, \mathrm{SO(3)}, \mathcal G_{2,4}$.
% In Figure~\ref{fig:t_experiment} we show some results for the two-dimensional torus. 

\textbf{Penalty Parameter.}
Intuitively, the minimizers of \eqref{eq:constant_speed_min_relaxed} will treat both terms almost equally, i.e., for $N\to \infty$ both terms are of the same order. Hence, our heuristic is to chose the parameter $\alpha$, such that the objective function and the penalty term are of the same order for minimizers of \eqref{eq:constant_speed_min_relaxed} i.e.,
\begin{equation}
  \label{eq:penalty_behavior}
  \frac{\alpha}{N} \sum_{k=1}^{N} \big(N \mathrm{dist}(x_{k-1},x_{k}) - L \big)_{+}^{2} \sim N^{-\frac{2s}{d}} \sim
  \min_{x_{1},\dots,x_{N}} \mathscr{D}^{2} \Big(\mu, \frac{1}{N} \sum_{k=1}^{N} \delta_{x_{k}}\Big) 
\end{equation}
Assuming that for the length $\mathcal L(\gamma) = \sum_{k=1}^{N}\mathrm{dist}(x_{k-1},x_{k})$ of a minimizer $\gamma$ we have $\mathcal L(\gamma) \sim L \sim N^{\frac{d-1}{d}}$, so that $N \mathrm{dist}(x_{k-1},x_{k}) \approx L$, then the value of the penalty term behaves like
\[
  \frac{\alpha}{N} \sum_{k=1}^{N} \big(N \mathrm{dist}(x_{k-1},x_{k}) - L \big)_{+}^{2} \sim  \alpha L^{2} \sim \alpha N^{\frac{2d-2}{d}}
\]
So that a reasonable choice is
\[
  \alpha \sim L^{-\frac{2s}{d-1}-2} \sim N^{\frac{-2s-2d+2}{d}}.
\]
%In Figure~\ref{fig:L_alpha_experiment}  we show some results for the two-dimensional torus. 


\subsection{Finding good curves of increasing length}

Since the objective function in \eqref{eq:constant_speed_min_relaxed} is highly non-convex, the main problem is to find good initial curves $\gamma_{L} \in  \mathcal P_{L}^{\Lcurve}(\mathbb X)$ for $L$ large. Our heuristic is as follows. We start with a curve $\gamma_{L_{0}}:[0,1]\to \mathbb X$ of small length $\mathcal L(\gamma) \approx L_{0}$ and solve the problem \eqref{eq:constant_speed_min_relaxed} for increasing $L_{i} = c L_{i-1}$, $c>1$, where we take the parameters $N_{i}$, $\alpha_{i}$ and $t_{i}$ in dependency of $L_{i}$ as described above.
% $N \sim L^{\frac{d}{d-1}}$, $\alpha \sim L^{-\frac{2(s-d-1)}{d-1}}$, $t \sim L^{d-1}$.

\textbf{2d-torus.}
We choose the following dependencies $\alpha = L^{-4}$, $N =11 L^{2}$, $t=3 L$. We start with a circle of radius $r=\frac13$ and solve \eqref{eq:constant_speed_min_relaxed} for $L=3$ using the conjugate gradient method. The obtained minimizer is used as initial point for the next minimization problem, where the parameter $L$ is doubled. The procedure is stopped when the twice the length of the minimizer bigger than the prescribed length.

%\bibliographystyle{abbrv}
%\bibliography{biblio_ehler2}
\end{document}
