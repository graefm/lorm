%\documentclass[12pt,a4paper]{amsart}
%!TEX TS-options = --shell-escape
\pdfobjcompresslevel=0% fix PDF inclusions
\documentclass[draft,
a4paper,11pt,DIV=11,% decrease borders by 2 (std 10 for 11pt)
abstract=on% return to old style centered abstract
]{scrartcl}
%
\usepackage{algorithm, booktabs}
\usepackage{mathtools}
\mathtoolsset{showonlyrefs}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{faktor}
\usepackage{amscd}
\usepackage{array}
%\usepackage{refcheck}\norefnames
\usepackage{amsthm}
\usepackage{subfigure}
\usepackage{mathdots}
\usepackage{mathrsfs}
\usepackage[noend]{algpseudocode}
\usepackage{graphicx}
\usepackage{color}
\usepackage{pgf}
\usepackage{scrpage2}
\usepackage{multirow}
\usepackage{listings}
\lstset{language=matlab,showstringspaces=false,basicstyle={\ttfamily}}
\usepackage{tikz}
\usepackage{graphicx}
\usepackage[left=2.5cm, right=2.5cm,top=2.5cm,bottom=2.5cm]{geometry}
\usepackage{scalerel}

\DeclareMathOperator{\Lcurve}{L-curv}
\newcommand{\TSP}{\text{TSP}}


\newtheorem{thm}{Theorem}[section]
\newtheorem{lemma}[thm]{Lemma}
\newtheorem{remark}[thm]{Remark}
\newtheorem{definition}[thm]{Definition}
\newtheorem{example}[thm]{Example}
\newtheorem{corollary}[thm]{Corollary}
\newtheorem{proposition}[thm]{Proposition}

%---------------------------------------------------------------------------
\begin{document}
\title{Curve Based Approximation of Measures on Manifolds by Discrepancy Minimization}

\author{
Martin Ehler\footnotemark[1],
\and
Manuel Gr\"af\footnotemark[2],
\and
Sebastian Neumayer\footnotemark[3]
\and
and Gabriele Steidl\footnotemark[3]
}
\maketitle

\date{\today}


\footnotetext[3]{Department of Mathematics,
	TU Kaiserslautern,
	Paul-Ehrlich-Str.~31, D-67663 Kaiserslautern, Germany,
	\{neumayer,steidl\}@mathematik.uni-kl.de
	}
\footnotetext[1]{University of Vienna, Department of Mathematics, Vienna, Austria,
	\{martin.ehler\}@univie.ac.at
	}
\footnotetext[2]{Austrian Academy of Sciences, Acoustics Research Institute, Vienna, Austria,
	\{mgraef\}@kfs.oeaw.ac.at
	}

\begin{abstract}
	\noindent\small
	

\end{abstract}

\section{Main problem}

We like to solve
\begin{equation}
  \label{eq:min_PL}
  \inf_{\nu} \mathscr D(\nu,\mu) \qquad \text{subject to} \qquad \nu \in \mathcal P_{L}^{\Lcurve}(\mathbb X).
\end{equation}
%respectively the relaxed optimization problem
%\begin{equation}
%  \label{eq:min_PL_relaxed}
%  \min_{\gamma \in A_{L}} \mathscr D^{2}(\gamma_{*}(\mathrm dt),\mu) + \alpha \int_{0}^{1} (\|\dot \gamma(t)\|-L)_{+}^{2} \mathrm dt.
%\end{equation}

\section{Numerical Approach}

For fixed $L>0$ and probability measure $\mu$ on $\mathbb X$ we aim to solve the minimization problem
\begin{equation}
  \label{eq:constant_speed_min}
  \min_{x_{1},\dots,x_{N}} \mathscr{D}^{2} \Big(\mu, \frac{1}{N} \sum_{k=1}^{N} \delta_{x_{k}}\Big) \qquad \mathrm{s.t.} \qquad    N \mathrm{dist}(x_{k},x_{k-1}) \le L, \quad k=1,\dots,N.
\end{equation}
Letting $N$ tend to infinity we eventually approach a closed curve $\gamma$, $\gamma(\tfrac kN) \approx x_{k}$ of speed at most $L$ such that the push-forward $\nu = \gamma_{*}(\mathrm dt)$, approaches $\mu$.  

We shall solve the optimization problem approximately by applying the nonlinear conjugate gradient method to the following unconstrained formulation
\begin{equation}
  \label{eq:constant_speed_min_relaxed}
  \min_{x_{1},\dots,x_{N}} \mathscr{D}^{2} \Big(\mu, \frac{1}{N} \sum_{k=1}^{N} \delta_{x_{k}}\Big) +   \frac{\alpha}{N} \sum_{k=1}^{N} \big(N \mathrm{dist}(x_{k-1},x_{k}) - L \big)_{+}^{2}
\end{equation}
where $\alpha > 0$ is a penalty parameter. Note, for $\alpha \to \infty$ there is a subsequence of minimizers of \eqref{eq:constant_speed_min_relaxed} which will converge to minimizers of \eqref{eq:constant_speed_min}.

\begin{remark}
  Obviously the objective function in \eqref{eq:constant_speed_min_relaxed} is not once and twice differentiable whenever $x_{k-1}=x_{k}$ and $\mathrm{dist}(x_{k-1},x_{k}) = \frac{L}{N}$, respectively. However, we observe numerically that local minimizers of \eqref{eq:constant_speed_min_relaxed} do not belong to this critical set of measure zero. That means in turn, if a local minimizer has a positive definite Hessian, then there is a local neighborhood where the CG method (with exact line search) permits a superlinearly convergence rate. We do indeed observe this behavior.
\end{remark}
For large $L$ we cannot hope to find a solution/global minimizer to problem \eqref{eq:constant_speed_min_relaxed}, since the number of local minimizers appears to increase exponentially {\color{blue}[ ref???]}.

\subsection{Numerical evaluation}
Recall that for given kernels $K:\mathbb X\times \mathbb X \to \mathbb R$ the squared discrepancy can be evaluated by
\[
  \mathscr D^{2}\Big(\mu, \frac 1N \sum_{k=1}^{N} \delta_{x_{k}}\Big) = E_{\mu}(x_{1},\dots,x_{N})
\]
where
\begin{equation}
  \label{eq:E_mu}
  E_{\mu}(x_{1},\dots,x_{N}) :=
    \frac{1}{N^{2}}\sum_{i,j=1}^{N} K(x_{i},x_{j}) - 2\sum_{i=1}^{N} \int_{\mathbb X} K(x_{i},x) \mathrm d\mu(x) + \int_{\mathbb X}\int_{\mathbb X} K(x,y) \mathrm d\mu(x) \mathrm d\mu(y).
  \end{equation}
Using the eigenfunction expansion
\[
 K(x,y) = \sum_{l=0}^{\infty} \lambda_{l} \varphi_{l}(x) \overline{\varphi_{l}(y)} 
\]
we have the alternative evaluation formula
\begin{equation}
  \label{eq:E_mu_Fourier}
  E_{\mu}(x_{1},\dots,x_{N}) = \sum_{l=0}^{\infty} \lambda_{l} \Big| \hat\mu_{l}
  - \frac 1N \sum_{i=1}^{N} \varphi_{l}(x_{i}) \Big|^{2}, \quad\quad \mu_{l} := \int_{\mathbb X} \varphi_{l}(x) \mathrm d\mu(x).
\end{equation}
Both formulas have it pros and cons:
\begin{itemize}
\item The formula \eqref{eq:E_mu} allows for exact evaluation only if the expression
\[
  K_{\mu}(x) := \int_{\mathbb X} K(x,y) \mathrm d\mu(y), \qquad  c_{\mu} := \int_{\mathbb X} K_{\mu}(x) \mathrm d\mu(x),
\]
can be written in closed form. In that case the complexity scales quadratically in the number $N$ of points.

\item The formula \eqref{eq:E_mu_Fourier} allows for exact evaluation only if the kernel has a finite expansion. In that case the complexity scales linearly in the number $N$ of points. 
\end{itemize}
\subsection{Choice of Parameters}

In order to find reasonable (local) solutions of \eqref{eq:min_PL} we shall carefully adjust the parameters in problem \eqref{eq:constant_speed_min_relaxed}, namely the penalty parameter $\alpha$, the number of points $N$ and the polynomial degree $t$.

\textbf{Number of Points.}
From the asymptotic of the path lengths of the traveling sales man problem (TSP) {\color{blue}(see aslo asymptotic for atomic/empirical measures)}
we know that we should choose
\[
  N \gtrsim \mathcal L(\gamma)^{\frac{d_{\mu}}{d_{\mu}-1}}, \qquad  {\color{red} d_{\mu} := \mathrm{dim}(\mathrm{supp}(\mu))} \ge 2
\]
where
\[
  \mathcal L(\gamma) = \sum_{k=1}^{N} \mathrm{dist}(x_{k-1},x_{k}) \le L
\]
is the length of the resulting curve $\gamma$ going through the points, i.e.,
\[
 \gamma(t_{k}) := x_{k}, \qquad t_{k} = \sum_{i=1}^{k}  \mathrm{dist}(x_{i-1},x_{i})/ \mathcal L(\gamma).
\]

%In Figure~\ref{fig:Nexperiment} we show some results for the two-dimensional torus. 

\textbf{Polynomial degree.} We believe that it is reasonable to take $r \sim N^{\frac{1}{d_{\mu}}} \sim L^{\frac{1}{d_{\mu}-1}}$.
% In Figure~\ref{fig:t_experiment} we show some results for the two-dimensional torus. 

\textbf{Penalty Parameter.}
Intuitively, the minimizers of \eqref{eq:constant_speed_min_relaxed} will treat both terms almost equally, i.e., for $N\to \infty$ both terms are of the same order. Hence, our heuristic is to chose the parameter $\alpha$, such that the objective function and the penalty term are of the same order for minimizers of \eqref{eq:constant_speed_min_relaxed} i.e.,
\begin{equation}
  \label{eq:penalty_behavior}
  \frac{\alpha}{N} \sum_{k=1}^{N} \big(N \mathrm{dist}(x_{k-1},x_{k}) - L \big)_{+}^{2}  \sim {\color{blue}  N^{-\frac{2s-(d-d_{\mu)}}{d_{\mu}}}} \sim
  \min_{x_{1},\dots,x_{N}} \mathscr{D}^{2} \Big(\mu, \frac{1}{N} \sum_{k=1}^{N} \delta_{x_{k}}\Big) 
\end{equation}
Assuming that for the length $\mathcal L(\gamma) = \sum_{k=1}^{N}\mathrm{dist}(x_{k-1},x_{k})$ of a minimizer $\gamma$ we have $\mathcal L(\gamma) \sim L \sim N^{\frac{d_{\mu}-1}{d_{\mu}}}$, so that $N \mathrm{dist}(x_{k-1},x_{k}) \approx L$, then the value of the penalty term behaves like
\[
  \frac{\alpha}{N} \sum_{k=1}^{N} \big(N \mathrm{dist}(x_{k-1},x_{k}) - L \big)_{+}^{2} \sim  \alpha L^{2} \sim \alpha N^{\frac{2d_{\mu}-2}{d_{\mu}}}
\]
So that a reasonable choice is
\[
  \alpha \sim L^{\frac{-2s-3d_{\mu}+d+2}{d_{\mu}-1}} \sim N^{\frac{-2s-3d_{\mu}+d+2}{d_{\mu}}}.
\]
%In Figure~\ref{fig:L_alpha_experiment}  we show some results for the two-dimensional torus. 


\begin{remark}
  In the case $r\sim N^{\frac{1}{d_{\mu}}} \sim L^{\frac{1}{d_{\mu}-1}}$ the overall computational cost of the discrepancy evaluation is almost $N^{\frac{d}{d_{\mu}}} \sim L^{\frac{d}{d_{\mu}-1}}$ for $\mathbb X = \mathbb T^{d}, \mathbb S^{2}, \mathrm{SO(3)}, \mathcal G_{2,4}$. That means, whenever $d_{\mu}=d$ the computational cost is almost linearly in $N \sim L^{\frac{d}{d-1}}$ in these specific examples. In contrast to $d_{\mu} = 2$, where the computational cost is almost $N^{\frac d2} \sim L^{d}$, we have a course of dimensions, which resides in the well-known fact that Fourier methods works best on the whole space $\mathbb X$.
\end{remark}



\subsection{Finding good curves of increasing $L$}

Since the objective function in \eqref{eq:constant_speed_min_relaxed} is highly non-convex, the main problem is to find good initial curves $\gamma_{L} \in  \mathcal P_{L}^{\Lcurve}(\mathbb X)$ for large $L$. Our heuristic is as follows. We start with a curve $\gamma_{L_{0}}:[0,1]\to \mathbb X$ of small length $\mathcal L(\gamma) \approx L_{0}$ and solve the problem \eqref{eq:constant_speed_min_relaxed} for increasing $L_{i} = c L_{i-1}$, $c>1$, where we take the parameters $N_{i}$, $\alpha_{i}$ and $t_{i}$ in dependency of $L_{i}$ as described above. Finally, we increase $\alpha$ to get a curve satisfying $L(\gamma) \le L$. 
% $N \sim L^{\frac{d}{d-1}}$, $\alpha \sim L^{-\frac{2(s-d-1)}{d-1}}$, $t \sim L^{d-1}$.

\textbf{2d-torus.}
In this example we show how well a gray-valued image may be approximated by an almost constant speed curve. The image is depicted in Figure... It has pixel size 170x170. The Fourier coefficients $\hat \mu_{k_{1},k_{2}}$ of the image are computed by a discrete Fourier transform (DFT) using the FFT algorithm and normalized appropriately. The kernel $K$ is given in Example~{\color{blue} TODO}, so that $H_{K} = H^{\frac32}(\mathbb T^{2})$.

We start with $N_{0}=96$ points on a circle given by the formula
\[
  x_{0,k} = ( \tfrac15 \cos(2\pi k/N), \tfrac15 \sin(2\pi k/N)), \qquad k=0,\dots,N_{0}.
\]
Then we solve for any index $i=0,\dots,11$ the problem \eqref{eq:constant_speed_min_relaxed} with parameters
\[
  L_{i} = 0.97\cdot 2^{\frac{i+5}{2}},\qquad \alpha_{i} = 100\,L_{i}^{-5},\qquad N_{i} = 2^{i} N_{0} \sim L_{i}^{2}, \qquad r_{i}= \lfloor 2^{\frac{i+11}{2}} \rfloor \sim L_{i}.
\]
The parameters are chosen such that
for a local minimizer $\gamma_{i}$ the length satisfies $\mathcal L(\gamma_{i}) \approx 2^{\frac{i+5}{2}}$ and the maximal speed is not too far from $L_{i}$. In each step a local minimizer is computed using the CG method with 100 interations. The obtained minimizer $\gamma_{i}$ will serve as the initial guess in the next step, which is obtained by inserting the midpoints, i.e. $x_{i+1,2k}=x_{i,k}$ and $x_{i+1,2k+1} = \frac{1}{2}(x_{i,k}+ x_{i,k+1})$, $k=0,\dots,N_{i}$. 

In order to get a curve of almost constant speed we increase $\alpha_{i}$ by a factor of 100 and set $L_{i} = 2^{\frac{i+5}{2}}$. Then we apply $(i+1)$ times the CG method with maximal 100 iterations to the previously found curve $\gamma_{i}$. The results are depicted in Figure... 

\textbf{3d-torus.} In this example we like to show two things. First, the algorithm works pretty well in three dimensions. Second we are able to approximate any compact surface in three-dimensional space by a curve {\color{blue} (Even more, the numerical experiments indicate that the approximation rates may indeed hold for any smooth Riemannian manifold)}.  

In this example we build a measure $\mu$ supported almost on a two-dimensional surface. More precisely, we take samples from Spock's head (see the Mingle homepage http://www.cs.technion.ac.il/$\sim$vitus/mingle/ for the data) and placing small Gaussian peaks at the sampling points, i.e., the density is given by
\[
  \rho(x) := c^{-1} \sum_{p \in X} \mathrm{e}^{-30000 \|p-x\|_2^2}, \quad x \in [-\tfrac12,\tfrac12], \qquad c := \int_{[-\tfrac12,\tfrac12]^3} \sum_{p \in X} \mathrm{e}^{-30000 \|p-x\|_2^2} \mathrm dx,
\]
where $X\subset \mathbb R^{3}$ is the sampling set. Here the Fourier coefficients are again computed by a DFT and the kernel $K$ is given in Example~{\color{blue} TODO}, so that $H_{K} = H^{2}(\mathbb T^{3})$.



%\bibliographystyle{abbrv}
%\bibliography{biblio_ehler2}
\end{document}
